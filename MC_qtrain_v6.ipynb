{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2db225",
   "metadata": {},
   "source": [
    "# Challenge: Quantum Natural Language Processing with lambeq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b471da",
   "metadata": {},
   "source": [
    "## Womanium Quantum Hackathon 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b00bd",
   "metadata": {},
   "source": [
    "## 0. Basic Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075184a",
   "metadata": {},
   "source": [
    "- **Team Name**: QThinkers\n",
    "- **Member & Contributions**:\n",
    "    - **Henry Zhiyu Ma (presenter)**: Coding & Writing this notebook (95%)\n",
    "    - Dimple Mevada: Discussion on algorithms (5%)\n",
    "    - Isabel Huh: None (0%)\n",
    "    - Constantin Drabo: None (0%)\\\n",
    "- **Contact Information**:\n",
    "    - Mr. Henry Zhiyu Ma: [Email](mailto:henry.zhiyu.ma@gmail.com), [GitHub](https://github.com/YujiUgaki), [LinkedIn](https://www.linkedin.com/in/henry-z-ma/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded6ae1",
   "metadata": {},
   "source": [
    "## 1. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c0f9b",
   "metadata": {},
   "source": [
    "This project aims at implementing **quantum algorithms** for **natural language processing (NLP)**, specifically, to determine whether a pair of short sentences belong to the same topic.\n",
    "\n",
    "Although a similar algorithm has been developed [1](https://cqcl.github.io/lambeq/tutorials/trainer_quantum.html), it only deals with one sentence each time. Therefore, we followed its quantum training pipeline while customizing the original `NumpyModel` to adapt it to sentence pair processing. We rewrite the `forward` method, adding the following features to it:\n",
    "  - accept **two lists** of circuits as input\n",
    "  - calculate the **cosine value** [2](https://arxiv.org/abs/1910.09129) of the feature vectors for each sentence in a pair\n",
    "With this custom method, we have successfully trained the model.\n",
    "\n",
    "Future improvements:\n",
    "  - New reading and parsing methods\n",
    "  - New similarity metrics for better performance\n",
    "  - Creating circuits using different prameters of ansatz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5924c3",
   "metadata": {},
   "source": [
    "## 2. Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4692511",
   "metadata": {},
   "source": [
    "### Step 0: preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7692791",
   "metadata": {},
   "source": [
    "Preparing & importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fd6f85-5418-4125-a458-c07eaf6d9775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\lambeq\\text2diagram\\ccg_parser.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lambeq import BobcatParser, AtomicType, IQPAnsatz, remove_cups, NumpyModel, QuantumTrainer, SPSAOptimizer, Dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61055b",
   "metadata": {},
   "source": [
    "Defining parameter for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01240713-b6b2-4b78-9faf-4f9f80c9e327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30 # batch size\n",
    "EPOCHS = 1000 # number of iterations\n",
    "SEED = 2\n",
    "eps = 1e-6 # a small number to get rid of 'NaN' value in 'log' function of binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad71f6",
   "metadata": {},
   "source": [
    "### Step 1: importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b997e",
   "metadata": {},
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e9520c-4238-482b-9a9f-96ff18d8892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    labels, sentences = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            \n",
    "            # extract & append labels\n",
    "            t = float(line[-2])\n",
    "            labels.append(t)\n",
    "            \n",
    "            # extract and sentences (Sentence 0 & 1 as a whole)\n",
    "            sentences.append(line[:-4])\n",
    "            \n",
    "    return labels, sentences\n",
    "\n",
    "# input data\n",
    "# MC1.txt is in the same folder as this jupyter notebook\n",
    "train_test_labels, train_test_data = read_data('./MC1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ace4e0",
   "metadata": {},
   "source": [
    "Randomly splitting the whole dataset into **train set (0.8)** and **test set (0.2)**, using `sklearn.model_selection import train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23aedc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(train_test_data, train_test_labels, test_size=0.2, random_state=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8f00a",
   "metadata": {},
   "source": [
    "Splitting the 2 sentences in each line apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef304ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    data0, data1 = [], []\n",
    "    for sentence in data:\n",
    "        data0.append(sentence.split(', ')[0])\n",
    "        data1.append(sentence.split(', ')[1])\n",
    "    return data0, data1\n",
    "\n",
    "train_data0, train_data1 = split_data(train_data)\n",
    "test_data0, test_data1 = split_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430cf30",
   "metadata": {},
   "source": [
    "### Step 2: creating, filtering, and simplifying diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ae963",
   "metadata": {},
   "source": [
    "**Parsing** sentences with `BobcatParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f5e59e-f1d9-49b0-b0e7-f5174258969d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n",
      "Tagging sentences.\n",
      "Parsing tagged sentences.\n",
      "Turning parse trees to diagrams.\n"
     ]
    }
   ],
   "source": [
    "parser = BobcatParser(root_cats=('NP', 'N'), verbose='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a46e3",
   "metadata": {},
   "source": [
    "Converting each sentence to **diagrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4877113",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_diagrams0 = parser.sentences2diagrams(train_data0, suppress_exceptions=True) # the 0th sentences in train set\n",
    "raw_train_diagrams1 = parser.sentences2diagrams(train_data1, suppress_exceptions=True) # the 1st sentences in train set\n",
    "raw_test_diagrams0 = parser.sentences2diagrams(test_data0, suppress_exceptions=True) # the 0th sentences in test set\n",
    "raw_test_diagrams1 = parser.sentences2diagrams(test_data1, suppress_exceptions=True) # the 1st sentences in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23997d06",
   "metadata": {},
   "source": [
    "Transforming diagrams to **normal forms**, simultaneously eliminating empty diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cc51b2-3c3f-4127-a207-3165a9270689",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diagrams0 = [\n",
    "    diagram.normal_form()\n",
    "    for diagram in raw_train_diagrams0 if diagram is not None\n",
    "]\n",
    "train_diagrams1 = [\n",
    "    diagram.normal_form()\n",
    "    for diagram in raw_train_diagrams1 if diagram is not None\n",
    "]\n",
    "test_diagrams0 = [\n",
    "    diagram.normal_form()\n",
    "    for diagram in raw_test_diagrams0 if diagram is not None\n",
    "]\n",
    "test_diagrams1 = [\n",
    "    diagram.normal_form()\n",
    "    for diagram in raw_test_diagrams1 if diagram is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a8e1d",
   "metadata": {},
   "source": [
    "Deleting the labels corresponsing to empty diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a10cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [\n",
    "    label for (diagram0, diagram1, label)\n",
    "    in zip(raw_train_diagrams0, raw_train_diagrams1, train_labels)\n",
    "    if diagram0 is not None and diagram1 is not None]\n",
    "test_labels = [\n",
    "    label for (diagram0, diagram1, label)\n",
    "    in zip(raw_test_diagrams0, raw_test_diagrams1, test_labels)\n",
    "    if diagram0 is not None and diagram1 is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a7ed9",
   "metadata": {},
   "source": [
    "### Step 3: creating circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385e105",
   "metadata": {},
   "source": [
    "Defining **ansatz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b9d17c0-1f2b-4bae-99c0-add79312ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz = IQPAnsatz({AtomicType.NOUN: 1, AtomicType.SENTENCE: 0},\n",
    "                   n_layers=1, n_single_qubit_params=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59696b6",
   "metadata": {},
   "source": [
    "**Removing the cups** of the diagrams to boost efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d3af9",
   "metadata": {},
   "source": [
    "Converting diagrams to **circuits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68586102",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_circuits0 = [ansatz(remove_cups(diagram)) for diagram in train_diagrams0]\n",
    "train_circuits1 = [ansatz(remove_cups(diagram)) for diagram in train_diagrams1]\n",
    "test_circuits0 =  [ansatz(remove_cups(diagram))  for diagram in test_diagrams0]\n",
    "test_circuits1 =  [ansatz(remove_cups(diagram))  for diagram in test_diagrams1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831cc96",
   "metadata": {},
   "source": [
    "### Step 4: defining custom model and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1ca8c",
   "metadata": {},
   "source": [
    "Defining a custom **subclass** `MyCustomModel` from `NumpyModel` with a modified `forward` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639e267",
   "metadata": {},
   "source": [
    "The `forward` method evaluates the **cosine** value of the **feature vectors** (outputs) of each sentence in a pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "992bca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomModel(NumpyModel):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "    \n",
    "    def forward(self,input):\n",
    "        \"\"\"This forward pass method has been customized for 2-sentence comparison\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : 2-dimensional list of :py:class:`Circuits <discopy.quantum.circuit.Circuit>` object\n",
    "                I.e., a list of 2-circuit pairs of sentences\n",
    "                The 0th dimension = BATCH_SIZE\n",
    "                The 1st dimension = 2\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Array containing model's prediction (y_hat)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # getting a 1-dimensional list of circuits for sentences0 & sentences1 from input\n",
    "        # since self.get_diagram_output(x) only accepts 1-dimensional list x\n",
    "        input0 = [input[i][0] for i in range(len(input))]\n",
    "        input1 = [input[i][1] for i in range(len(input))]\n",
    "        \n",
    "        # evaluating feature vectors for sentences0 & sentences1\n",
    "        preds0 = self.get_diagram_output(input0) # a list of 2-dimensional vectors [_,_]\n",
    "        preds1 = self.get_diagram_output(input1)\n",
    "        if len(preds0) != len(preds1):\n",
    "            raise ValueError('Lengths of `preds0` and `preds1` differ.')\n",
    "        \n",
    "        # evaluating the cosine of the angle between the feature vectors for sentences0 & sentences1\n",
    "        # supposing that cosine value measures similarity\n",
    "        # ref: arXiv:1910.09129\n",
    "        def vcos(v0, v1):\n",
    "            return np.divide(np.dot(v0, v1), np.sqrt(np.multiply(np.dot(v0,v0), np.dot(v1,v1))))\n",
    "        \n",
    "        # list of cosine values for each pair of sentences\n",
    "        coss = [vcos(preds0[i], preds1[i]) for i in range(len(preds0))]\n",
    "        \n",
    "        # converting cosine list to array\n",
    "        preds = np.array(coss)\n",
    "        return preds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0d17b",
   "metadata": {},
   "source": [
    "Building model: `model.symbols` is generated from all words present in all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24feed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_circuits = train_circuits0 + test_circuits0 + train_circuits1 + test_circuits1\n",
    "\n",
    "# use jit to accelerate training\n",
    "model = MyCustomModel.from_diagrams(all_circuits, use_jit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c744f61",
   "metadata": {},
   "source": [
    "Defining **loss** and **accuracy** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b0030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda y_hat, y: -np.sum(y * np.log(y_hat + eps)) / len(y)  # binary cross-entropy loss\n",
    "\n",
    "acc = lambda y_hat, y: np.sum(np.round(y_hat) == y) / len(y) / 2  # half due to double-counting\n",
    "etest_metrics = {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a62e4b",
   "metadata": {},
   "source": [
    "### Step 5: initializing trainer and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42bf39",
   "metadata": {},
   "source": [
    "Setting up **trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9687a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=loss,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.05, 'c': 0.06, 'A':0.01*EPOCHS},\n",
    "    evaluate_functions=etest_metrics,\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5056c2",
   "metadata": {},
   "source": [
    "Obtaining **datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bed49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of circuit-pairs\n",
    "train_circuits = [list(item) for item in zip(train_circuits0, train_circuits1)]\n",
    "test_circuits = [list(item) for item in zip(test_circuits0, test_circuits1)]\n",
    "\n",
    "train_dataset = Dataset(\n",
    "            train_circuits,\n",
    "            train_labels,\n",
    "            batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = Dataset(test_circuits, test_labels, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4961e0",
   "metadata": {},
   "source": [
    "**Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efe0d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:     train/loss: 0.0912   valid/loss: 0.0811   train/acc: 0.2562   valid/acc: 0.3000\n",
      "Epoch 2:     train/loss: 0.0829   valid/loss: 0.0803   train/acc: 0.2562   valid/acc: 0.3000\n",
      "Epoch 3:     train/loss: 0.0834   valid/loss: 0.0835   train/acc: 0.2625   valid/acc: 0.3000\n",
      "Epoch 4:     train/loss: 0.0692   valid/loss: 0.0831   train/acc: 0.2562   valid/acc: 0.3000\n",
      "Epoch 5:     train/loss: 0.0681   valid/loss: 0.0840   train/acc: 0.2562   valid/acc: 0.3000\n",
      "Epoch 6:     train/loss: 0.0722   valid/loss: 0.0782   train/acc: 0.2625   valid/acc: 0.3000\n",
      "Epoch 7:     train/loss: 0.0728   valid/loss: 0.0786   train/acc: 0.2562   valid/acc: 0.3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\lambeq\\training\\trainer.py:371\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, train_dataset, val_dataset, evaluation_step, logging_step)\u001b[0m\n\u001b[0;32m    369\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    370\u001b[0m x, y_label \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m--> 371\u001b[0m y_hat, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_on_train\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_functions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metr, func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_functions\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\lambeq\\training\\quantum_trainer.py:153\u001b[0m, in \u001b[0;36mQuantumTrainer.training_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    139\u001b[0m         batch: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Any], np\u001b[38;5;241m.\u001b[39mndarray]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform a training step.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     y_hat, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_costs\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\lambeq\\training\\spsa_optimizer.py:138\u001b[0m, in \u001b[0;36mSPSAOptimizer.backward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    136\u001b[0m xminus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject(x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mck \u001b[38;5;241m*\u001b[39m delta)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m xminus\n\u001b[1;32m--> 138\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiagrams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y1, targets)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\lambeq\\training\\model.py:64\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mMyCustomModel.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m      6\u001b[0m input0 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m))]\n\u001b[0;32m      7\u001b[0m input1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m))]\n\u001b[1;32m----> 9\u001b[0m preds0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_diagram_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m preds1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_diagram_output(input1)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds0) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(preds1):\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\lambeq\\training\\numpy_model.py:172\u001b[0m, in \u001b[0;36mNumpyModel.get_diagram_output\u001b[1;34m(self, diagrams)\u001b[0m\n\u001b[0;32m    170\u001b[0m diagrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_subs(diagrams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Tensor\u001b[38;5;241m.\u001b[39mbackend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalise_vector(tn\u001b[38;5;241m.\u001b[39mcontractors\u001b[38;5;241m.\u001b[39mauto(\u001b[38;5;241m*\u001b[39md\u001b[38;5;241m.\u001b[39mto_tn())\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m diagrams])\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\lambeq\\training\\numpy_model.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m diagrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_subs(diagrams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Tensor\u001b[38;5;241m.\u001b[39mbackend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m--> 173\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalise_vector(\u001b[43mtn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontractors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m diagrams])\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\tensornetwork\\contractors\\opt_einsum_paths\\path_contractors.py:265\u001b[0m, in \u001b[0;36mauto\u001b[1;34m(nodes, output_edge_order, memory_limit, ignore_edge_order)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m15\u001b[39m:\n\u001b[0;32m    260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m branch(\n\u001b[0;32m    261\u001b[0m       nodes,\n\u001b[0;32m    262\u001b[0m       output_edge_order\u001b[38;5;241m=\u001b[39moutput_edge_order,\n\u001b[0;32m    263\u001b[0m       nbranch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    264\u001b[0m       ignore_edge_order\u001b[38;5;241m=\u001b[39mignore_edge_order)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgreedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_edge_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_edge_order\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\tensornetwork\\contractors\\opt_einsum_paths\\path_contractors.py:193\u001b[0m, in \u001b[0;36mgreedy\u001b[1;34m(nodes, output_edge_order, memory_limit, ignore_edge_order)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"Greedy contraction path via `opt_einsum`.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mThis provides a more efficient strategy than `optimal` for finding\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m  The final node after full contraction.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m alg \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(opt_einsum\u001b[38;5;241m.\u001b[39mpaths\u001b[38;5;241m.\u001b[39mgreedy, memory_limit\u001b[38;5;241m=\u001b[39mmemory_limit)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_edge_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_edge_order\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\tensornetwork\\contractors\\opt_einsum_paths\\path_contractors.py:88\u001b[0m, in \u001b[0;36mbase\u001b[1;34m(nodes, algorithm, output_edge_order, ignore_edge_order)\u001b[0m\n\u001b[0;32m     86\u001b[0m path, nodes \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_path(nodes_set, algorithm)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m path:\n\u001b[1;32m---> 88\u001b[0m   new_node \u001b[38;5;241m=\u001b[39m \u001b[43mcontract_between\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_outer_product\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m   nodes\u001b[38;5;241m.\u001b[39mappend(new_node)\n\u001b[0;32m     90\u001b[0m   nodes \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mmulti_remove(nodes, [a, b])\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\tensornetwork\\network_components.py:2085\u001b[0m, in \u001b[0;36mcontract_between\u001b[1;34m(node1, node2, name, allow_outer_product, output_edge_order, axis_names)\u001b[0m\n\u001b[0;32m   2083\u001b[0m axes1 \u001b[38;5;241m=\u001b[39m [axes1[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ind_sort]\n\u001b[0;32m   2084\u001b[0m axes2 \u001b[38;5;241m=\u001b[39m [axes2[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ind_sort]\n\u001b[1;32m-> 2085\u001b[0m new_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43maxes1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2086\u001b[0m new_node \u001b[38;5;241m=\u001b[39m Node(tensor\u001b[38;5;241m=\u001b[39mnew_tensor, name\u001b[38;5;241m=\u001b[39mname, backend\u001b[38;5;241m=\u001b[39mbackend)\n\u001b[0;32m   2087\u001b[0m \u001b[38;5;66;03m# node1 and node2 get new edges in _remove_edges\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\QuantumComputing\\lib\\site-packages\\tensornetwork\\backends\\numpy\\numpy_backend.py:53\u001b[0m, in \u001b[0;36mNumPyBackend.tensordot\u001b[1;34m(self, a, b, axes)\u001b[0m\n\u001b[0;32m     51\u001b[0m     einsum_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(labels_1), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(labels_2)])\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39meinsum(einsum_label, a, b, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m---> 53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtensordot(a, b, axes)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mtensordot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\numeric.py:1138\u001b[0m, in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes)\u001b[0m\n\u001b[0;32m   1136\u001b[0m at \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose(newaxes_a)\u001b[38;5;241m.\u001b[39mreshape(newshape_a)\n\u001b[0;32m   1137\u001b[0m bt \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mtranspose(newaxes_b)\u001b[38;5;241m.\u001b[39mreshape(newshape_b)\n\u001b[1;32m-> 1138\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mreshape(olda \u001b[38;5;241m+\u001b[39m oldb)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(train_dataset, test_dataset, evaluation_step=1, logging_step=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed028836",
   "metadata": {},
   "source": [
    "Plotting fitted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab896216",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(10, 6))\n",
    "ax_tl.set_title('Training set')\n",
    "ax_tr.set_title('Development set')\n",
    "ax_bl.set_xlabel('Iterations')\n",
    "ax_br.set_xlabel('Iterations')\n",
    "ax_bl.set_ylabel('Accuracy')\n",
    "ax_tl.set_ylabel('Loss')\n",
    "\n",
    "colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "ax_tl.plot(trainer.train_epoch_costs[::10], color=next(colours))\n",
    "ax_bl.plot(trainer.train_results['acc'][::10], color=next(colours))\n",
    "ax_tr.plot(trainer.test_costs[::10], color=next(colours))\n",
    "ax_br.plot(trainer.test_results['acc'][::10], color=next(colours))\n",
    "\n",
    "# print test accuracy\n",
    "test_acc = acc(model(test_circuits), test_labels)\n",
    "print('Validation accuracy:', test_acc.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
